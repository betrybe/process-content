## O que vamos aprender?

Hoje vamos aprender o que √© **raspagem de dados** e o que podemos fazer utilizando esta t√©cnica. Vamos ver tamb√©m como fazer requisi√ß√µes web, analisando suas respostas e extraindo dados. Al√©m disso, tamb√©m vamos ver t√©cnicas para evitar problemas com essa pr√°tica.

### Voc√™ ser√° capaz de:

- Realizar requisi√ß√µes web;

- Analisar conte√∫dos HTML afim de extrair dados;

- Aplicar t√©cnicas de raspagem para evitar problemas como bloqueio de acesso;

- Armazenar os dados obtidos em um banco de dados.

## Por que isso √© importante?

Imagine que voc√™ queira fazer compara√ß√µes de pre√ßos e informa√ß√µes, ou talvez descobrir informa√ß√µes sobre algum assunto. Todo o dado a respeito, por√©m, n√£o est√° estruturado, sendo exibido somente em uma p√°gina web. Uma p√°gina web pode ser √∫til para humanos, mas certamente n√£o √© √∫til para fazermos an√°lises estruturadas.

<%= figure(%{src: "/computer-science/python/raspagem-dados/images/not-a-robot.gif", class: "text-center rounded mx-auto d-block"}) %>

A **raspagem de dados** tem sido muito √∫til em trabalhos jornal√≠sticos, fornecendo dados para sustentarem mat√©rias, mas tamb√©m pode ser √∫til para comparar pre√ßos de produtos com a concorr√™ncia, automatiza√ß√£o de processos ma√ßantes como buscar artigos cient√≠ficos em bases acad√™micas, recupera√ß√£o de documentos em sites jur√≠dicos, analisar perfis de redes sociais, recuperar dados p√∫blicos do governo e muitos outros lugares.

> Caso tenha repetido um processo mais de duas vezes, automatize-o.

Dado a infinidade de usos provavelmente, em sua carreira como pessoa desenvolvedora, voc√™ ir√° se deparar em algum momento  com a necessidade de escrever um programa que realize a extra√ß√£o de dados para algum prop√≥sito.

---

## Conte√∫dos

###### Tempo sugerido para realiza√ß√£o: 80 minutos

### Introdu√ß√£o

**Raspagem de dados** √© uma t√©cnica computacional para extrair dados provenientes de um servi√ßo ou aplica√ß√£o, estruturando-os em banco de dados, planilhas, ou outros formatos. Consiste em extrair dados de sites e transport√°-los para um formato mais simples e male√°vel para que possam ser analisados e cruzados com mais facilidade.

Alguns passos aplicados nesta t√©cnica s√£o a realiza√ß√£o de requisi√ß√µes, an√°lise das resposta e extra√ß√£o dos dados, navega√ß√£o do conte√∫do, limpeza e armazenamento dos dados.

Vamos passo a passo, aprendendo como fazer e prevenindo erros que podem acontecer.

### Requisi√ß√µes web

A comunica√ß√£o com servidores HTTP e HTTPS se d√° atrav√©s de requisi√ß√µes, em que utilizamos o verbo para indicar que tipo de a√ß√£o deve ser tomada para um dado recurso. Devemos informar na requisi√ß√£o em qual recurso estamos atuando e no cabe√ßalho passamos algumas informa√ß√µes que podem ser interessantes como o tipo de resposta aceita ou o tipo de conte√∫do sendo enviado.

O **Python** possui um pacote para lidar com o protocolo HTTP, por√©m este n√£o √© t√£o amig√°vel para seres humanos. Por isso vamos utilizar a biblioteca `requests`, que possui uma interface bem mais amig√°vel. Ela pode ser instalada utilizando o comando abaixo, mas lembre-se de criar um ambiente virtual antes de instalar bibliotecas. Para recordar como criar um ambiente isolado, [acesse o conte√∫do](/computer-science/python/entrada-saida/#ambiente-virtual) {: .external-link target="_blank" rel="noreferrer noopener" }.

```language-bash
$ python3 -m pip install requests
```

Vamos ver alguns exemplos abaixo de como utilizar a biblioteca `requests`:

```language-python
import requests


# Requisi√ß√£o do tipo GET
response = requests.get("https://www.betrybe.com/")
print(response.status_code)  # c√≥digo de status
print(response.headers["Content-Type"])  # conte√∫do no formato html

# Conte√∫do recebido da requisi√ß√£o
print(response.text)

# Bytes recebidos como resposta
print(response.content)

# Requisi√ß√£o do tipo post
response = requests.post("http://httpbin.org/post", data="some content")
print(response.text)

# Requisi√ß√£o enviando cabe√ßalho (header)
response = requests.get("http://httpbin.org/get", headers={"Accept": "application/json"})
print(response.text)

# Requisi√ß√£o a recurso bin√°rio
response = requests.get("http://httpbin.org/image/png")
print(response.content)

# Recurso JSON
response = requests.get("http://httpbin.org/get")
# Equivalente ao json.loads(response.content)
print(response.json())

# Podemos tamb√©m pedir que a resposta lance uma exce√ß√£o caso o status n√£o seja OK
response = requests.get("http://httpbin.org/status/404")
response.raise_for_status()
```

### Alguns problemas

#### Rate Limit

Muitas vezes a p√°gina de onde estamos removendo o conte√∫do possui uma limita√ß√£o de quantas requisi√ß√µes podemos enviar em um curto per√≠odo de tempo, evitando assim ataques de nega√ß√£o de servi√ßo.

Isto pode nos levar a ficarmos bloqueados por um curto per√≠odo de tempo ou at√© mesmo banidos de acessar um recurso.

Que tal vermos um exemplo?

```language-python
import requests


# √Ä partir da d√©cima requisi√ß√£o somos bloqueados de acessar o recurso
# C√≥digo de status 429: Too Many Requests
for _ in range(15):
    response = requests.get("https://www.cloudflare.com/rate-limit-test/")
    print(response.status_code)
```

Neste exemplo, ap√≥s a d√©cima requisi√ß√£o, o servidor come√ßa a nos retornar respostas com o c√≥digo de `status 429`, "Too Many Requests". Isto significa que estamos utilizando uma taxa de solicita√ß√£o maior que a suportada. Ele permanecer√° assim por algum tempo (1 minuto).

<%= figure(%{src: "/computer-science/python/raspagem-dados/images/denial.gif", class: "text-center rounded mx-auto d-block"}) %>

Uma boa pr√°tica √© sempre colocarmos um uma pausa entre as requisi√ß√µes, ou lote delas, mesmo que o website, onde a informa√ß√£o est√°, n√£o fa√ßa o bloqueio, assim evitamos tirar o site do ar.

```language-python
import requests
import time


# Coloca uma pausa de 6 segundos a cada requisi√ß√£o
# Obs: este site de exemplo tem um rate limit de 10 requisi√ß√µes por minuto
for _ in range(15):
    response = requests.get("https://www.cloudflare.com/rate-limit-test/")
    print(response)
    time.sleep(6)
```

#### Timeout

√Ås vezes pedimos um recurso ao servidor, mas caso o nosso tr√°fego de rede esteja lento ou tenha um problema interno do servidor, nossa resposta pode demorar ou at√© mesmo ficar travada indefinidamente.

Como garantir, ap√≥s um tempo, que o recurso seja retornado? ü§î

```language-python
import requests

# Por 10 segundos n√£o temos certeza se a requisi√ß√£o ir√° retornar
response = requests.get("https://httpbin.org/delay/10")
print(response)
```

Podemos definir um **tempo limite (timeout)** para que, ap√≥s este tempo, possamos tomar alguma atitude como por exemplo, realizar uma nova tentativa.

Este tempo limite normalmente √© definido atrav√©s de experimenta√ß√µes e an√°lise do tempo de resposta normal de uma requisi√ß√£o.

```language-python
import requests


try:
    # recurso demora muito a responder
    response = requests.get("http://httpbin.org/delay/10", timeout=2)
except requests.ReadTimeout:
    # vamos fazer uma nova requisi√ß√£o
    response = requests.get("http://httpbin.org/delay/1", timeout=2)
finally:
    print(response.status_code)
```

No exemplo acima, para efeitos did√°ticos, modificamos a URI do recurso, diminuindo o delay de resposta da requisi√ß√£o, atrav√©s do `timeout`, por√©m normalmente este valor n√£o muda.

### Analisando respostas

> üóí Para realizar a extra√ß√£o de dados de um conte√∫do web vamos utilizar uma biblioteca chamada `parsel`. Ela pode ser instalada com o comando o comando abaixo:

```language-bash
$ python3 -m pip install parsel
```

Como j√° aprendemos a realizar requisi√ß√µes HTTP e buscar seu conte√∫do, vamos agora analisar este conte√∫do e extrair informa√ß√µes.

Para ajudar na did√°tica, vamos utilizar o site `http://books.toscrape.com/`. Ele √© um site pr√≥prio para o treinamento de raspagem de dados.

Para come√ßar, vamos acessar o site e ver o seu conte√∫do.

<%= figure(%{src: "/computer-science/python/raspagem-dados/images/books-to-scrape.png", caption: "Livros √† venda", class: "text-center rounded mx-auto d-block"}) %>

Em c√≥digo, vamos baixar o conte√∫do da p√°gina e criar um seletor, que ser√° utilizado para realizarmos as extra√ß√µes. Vamos criar o arquivo `.py` para buscarmos as informa√ß√µes:

> exemplo_scrape.py

```language-python
from parsel import Selector
import requests


response = requests.get("http://books.toscrape.com/")
selector = Selector(text=response.text)
print(selector)
```

_Ok_, que tal extrairmos todos os livros desta primeira p√°gina e buscar seus t√≠tulos e pre√ßos?

Para conseguirmos extrair essas informa√ß√µes precisamos, primeiro, inspecionar cada um dos elementos, buscando algo que os identifique de forma √∫nica na p√°gina.

<%= figure(%{src: "/computer-science/python/raspagem-dados/images/inspect-title.png", caption: "Inspecionando o t√≠tulo", class: "text-center rounded mx-auto d-block"}) %>

> exemplo_scrape.py

```language-python
# ...


# response = requests.get("http://books.toscrape.com/")
# selector = Selector(text=response.text)

# O t√≠tulo est√° no atributo title em um elemento √¢ncora (<a>)
# Dentro de um h3 em elementos que possuem classe product_pod
titles = selector.css(".product_pod h3 a::attr(title)").getall()
# Estamos utilizando a::attr(title) para capturar somente o valor contido no texto do seletor

# Os pre√ßos est√£o no texto de uma classe price_color
# Que se encontra dentro da classe .product_price
prices = selector.css(".product_price .price_color::text").getall()

# Combinando tudo podemos buscar os produtos
# em em seguida buscar os valores individualmente
for product in selector.css(".product_pod"):
    title = product.css("h3 a::attr(title)").get()
    price = product.css(".price_color::text").get()
    print(title, price)
```

O seletor principal que cont√©m todo o conte√∫do da p√°gina pode ser utilizado em uma busca para encontrar seletores mais espec√≠ficos. Podemos fazer isto utilizando a fun√ß√£o `css`. Ela recebe um seletor CSS como par√¢metro, embora podemos passar um tipo especial de seletor quando queremos algum valor bem espec√≠fico como o texto ou um atributo.

Ap√≥s definir o seletor, podemos utilizar a fun√ß√£o `get` para buscar o primeiro seletor/valor que satisfa√ßa aquela busca. A fun√ß√£o `getall` √© similar, por√©m tr√°s todos os valores que satisfa√ßam aquele seletor.

Assim como temos a fun√ß√£o `css` que faz a busca por seletores CSS, temos tamb√©m a fun√ß√£o `xpath` que faz a busca com base em XPath.

> üí° Existem sites que podem ajudar com seletores [css](https://devhints.io/css) ou [xpath](https://devhints.io/xpath).

### Recursos paginados

Tudo certo, temos 20 livros, mas sabemos que na verdade este site possui **1000** livros. Que tal coletarmos todos?!

<%= figure(%{src: "/computer-science/python/raspagem-dados/images/page-two.png", caption: "Navega√ß√£o at√© a p√°gina 2", class: "text-center rounded mx-auto d-block"}) %>

Navegando um pouco por entre as p√°ginas, percebemos que cada p√°gina possui uma refer√™ncia para a pr√≥xima. Mas, se a `URL` √© sequencial, por que n√£o simplesmente jogamos valores at√© a p√°gina avisar que o recurso n√£o foi encontrado?

Acontece que podemos evitar fazer requisi√ß√µes desnecess√°rias, j√° que a p√°gina nos informa a pr√≥xima.

O que precisamos fazer √© criar um seletor com a p√°gina, extrair os dados, buscar a nova p√°gina e repetir todo o processo, at√© termos navegados em todas as p√°ginas.

At√© a parte da extra√ß√£o n√≥s j√° fizemos, vamos ent√£o dar uma olhada em como buscar a pr√≥xima p√°gina.

<%= figure(%{src: "/computer-science/python/raspagem-dados/images/inspect-next.png", caption: "Inspecionando o bot√£o de pr√≥ximo", class: "text-center rounded mx-auto d-block", width: "100%"}) %>

> exemplo_scrape.py

```language-python
# ...
# for product in selector.css(".product_pod"):
#     title = product.css("h3 a::attr(title)").get()
#     price = product.css(".price_color::text").get()
#     print(title, price)

# Existe uma classe next, que podemos recuperar a url atrav√©s do seu elemento √¢ncora
next_page_url = selector.css(".next a::attr(href)").get()
print(next_page_url)
```

Agora que sabemos como recuperar a pr√≥xima p√°gina, podemos iniciar na primeira p√°gina e continuar buscando livros enquanto uma nova p√°gina for encontrada. Cada vez que encontrarmos uma p√°gina, extra√≠mos seus dados e continuamos at√© acabarem as p√°ginas. Ent√£o vamos alterar tudo que t√≠nhamos escrito no arquivo `exemplo_scrape.py` para o c√≥digo abaixo:

> exemplo_scrape.py

```language-python
from parsel import Selector
import requests


# Define a primeira p√°gina como pr√≥xima a ter seu conte√∫do recuperado
URL_BASE = "http://books.toscrape.com/catalogue/"
next_page_url = 'page-1.html'
while next_page_url:
    # Busca o conte√∫do da pr√≥xima p√°gina
    response = requests.get(URL_BASE + next_page_url)
    selector = Selector(text=response.text)
    # Imprime os produtos de uma determinada p√°gina
    for product in selector.css(".product_pod"):
        title = product.css("h3 a::attr(title)").get()
        price = product.css(".price_color::text").get()
        print(title, price)
    # Descobre qual √© a pr√≥xima p√°gina
    next_page_url = selector.css(".next a::attr(href)").get()
```

Nossa, quantos livros! üìö

### Recursos obtidos √† partir de outro recurso

Agora que estamos coletando todos os livros, que tal coletarmos tamb√©m a descri√ß√£o de cada livro?

O problema √© que a descri√ß√£o s√≥ pode ser acessada navegando at√© a p√°gina espec√≠fica de cada livro.

<%= figure(%{src: "/computer-science/python/raspagem-dados/images/book-description.png", caption: "Descri√ß√£o do livro est√° em sua p√°gina de detalhes", class: "text-center rounded mx-auto d-block", width: "100%"}) %>

O primeiro passo √© investigarmos como descobrir a `URL` que nos leva at√© a p√°gina de detalhes de um produto. üîç

<%= figure(%{src: "/computer-science/python/raspagem-dados/images/inspect-book-url.png", caption: "Descri√ß√£o do livro est√° em sua p√°gina de detalhes", class: "text-center rounded mx-auto d-block", width: "100%"}) %>

Com esse seletor em m√£os, precisamos recuperar o atributo `href` que cont√©m o link para a p√°gina de detalhes do livro. Vamos criar um outro arquivo, apenas para fazer o teste da _feature_ que queremos implementar, depois vamos alterar o c√≥digo do arquivo `exemplo_scrape.py` para realmente implementar a fun√ß√£o. Lembre-se de criar o arquivo no mesmo diret√≥rio que j√° estava utilizando.

> teste.py

```language-python
from parsel import Selector
import requests

URL_BASE = "http://books.toscrape.com/catalogue/"

# Vamos testar com a primeira p√°gina
response = requests.get(URL_BASE + "page-1.html")
selector = Selector(text=response.text)

# Recupera o atributo href do primeiro elemento que combine com o seletor
href = selector.css(".product_pod h3 a::attr(href)").get()
print(href)

# Para obter a url completa, basta adicionar a nossa URL_BASE
print(URL_BASE + href)
```

Em seguida, acessamos a p√°gina de detalhes do produto, e inspecionamos a descri√ß√£o do produto.

<%= figure(%{src: "/computer-science/python/raspagem-dados/images/inspect-book-description.png", caption: "Inspecionando a descri√ß√£o de um livro.", class: "text-center rounded mx-auto d-block", width: "100%"}) %>

A descri√ß√£o do produto est√° uma tag `p`, "irm√£" de uma tag com id `product_description`. Isto pode ser expresso atrav√©s do seletor `a`.

No c√≥digo, precisamos realizar uma nova requisi√ß√£o √† p√°gina de detalhes, e depois analisaremos seu conte√∫do em busca da descri√ß√£o do produto. Para isso, vamos alterar todo o conte√∫do novamente, por√©m dessa vez ser√° o conte√∫do do arquivo `teste.py`:

> teste.py

```language-python
from parsel import Selector
import requests

URL_BASE = "http://books.toscrape.com/catalogue/"

response = requests.get(URL_BASE + "page-1.html")
selector = Selector(text=response.text)

href = selector.css(".product_pod h3 a::attr(href)").get()
detail_page_url = URL_BASE + href

# baixa o conte√∫do da p√°gina de detalhes
detail_response = requests.get(detail_page_url)
detail_selector = Selector(text=detail_response.text)

# recupera a descri√ß√£o do produto
# ~ significa a tag irm√£
description = detail_selector.css("#product_description ~ p::text").get()
print(description)
```

Por fim, vamos adicionar a l√≥gica para buscar a descri√ß√£o do produto ao nosso c√≥digo existente.

> exemplo_scrape.py

```language-python
# from parsel import Selector
# import requests


# URL_BASE = "http://books.toscrape.com/catalogue/"
# Define a primeira p√°gina como pr√≥xima a ter seu conte√∫do recuperado
# next_page_url = 'page-1.html'
while next_page_url:
    # Busca o conte√∫do da pr√≥xima p√°gina
    # response = requests.get(URL_BASE + next_page_url)
    # selector = Selector(text=response.text)
    # Imprime os produtos de uma determinada p√°gina
    for product in selector.css(".product_pod"):
        # Busca e extrai o t√≠tulo e  o pre√ßo
        # title = product.css("h3 a::attr(title)").get()
        # price = product.css(".price_color::text").get()
        # print(title, price)

        # Busca o detalhe de um produto
        detail_href = product.css("h3 a::attr(href)").get()
        detail_page_url = URL_BASE + detail_href

        # Baixa o conte√∫do da p√°gina de detalhes
        detail_response = requests.get(detail_page_url)
        detail_selector = Selector(text=detail_response.text)

        # Extrai a descri√ß√£o do produto
        description = detail_selector.css("#product_description ~ p::text").get()
        print(description)

    # Descobre qual √© a pr√≥xima p√°gina
    # next_page_url = selector.css(".next a::attr(href)").get()
```

### Limpeza de dados

üßºüßΩ Estamos extraindo nossos dados, por√©m estes dados cont√©m algumas "sujeiras" que podem atrapalhar em nossas an√°lises. Por exemplo, pare pra olhar o pre√ßo do livro, viu algo diferente? O pre√ßo possui um caractere estranho `√Ç¬£26.08` antes do seu valor. E a descri√ß√£o do livro que cont√©m o sufixo `...more`.

Seria conveniente, antes de estruturar e armazenar estes dados, que fiz√©ssemos uma limpeza neles.

No caso do valor, poder√≠amos utilizar uma express√£o regular para remover os outros caracteres. O padr√£o √© conter um s√≠mbolo de libra, seguido por n√∫meros, ponto para separar casas decimais e dois n√∫meros como casas decimais. Essa express√£o regular pode ser feita da seguinte forma: `¬£\d+\.\d{2}`.

Agora, para utilizar a express√£o regular que fizemos, podemos substituir o m√©todo `getall` pelo m√©todo `re`, ou o m√©todo `get` por `re_first`. Ambos, al√©m de recuperar valores, aplicar√£o a express√£o regular sobre aquele valor.

Vamos primeiro fazer essas _features_, novamente, no arquivo de `teste.py` para vermos funcionando. Depois vamos implement√°-las no arquivo `exemplo_scrape.py`.

> teste.py

```language-python
from parsel import Selector
import requests


response = requests.get("http://books.toscrape.com/")
selector = Selector(text=response.text)
# Extrai todos os pre√ßos da primeira p√°gina
prices = selector.css(".product_price .price_color::text").re(r"¬£\d+\.\d{2}")
print(prices)
```

J√° para o caso do sufixo `...more`, poder√≠amos utilizar fatiamento para remov√™-lo. Mas, antes, √© importante verificarmos se o conte√∫do possui o sufixo, evitando assim perda de conte√∫do de forma acidental. Vamos ver como isso funciona no arquivo `teste.py`.

> teste.py

```language-python
from parsel import Selector
import requests


response = requests.get("http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html")
selector = Selector(text=response.text)

# Extrai a descri√ß√£o
description = selector.css("#product_description ~ p::text").get()
print(description)

# "Fatiamos" a descri√ß√£o removendo o sufixo
suffix = "...more"
if description.endswith(suffix):
    description = description[:-len(suffix)]
print(description)
```

Alguns outros exemplos de "sujeiras" s√£o valores que cont√©m tabula√ß√µes, quebras de linha ou conte√∫do al√©m do esperado.

Agora vamos implementar essas funcionalidades no nosso arquivo `exemplo_scrape.py`.

> exemplo_scrape.py

```language-python
from parsel import Selector
import requests


# URL_BASE = "http://books.toscrape.com/catalogue/"
# Define a primeira p√°gina como pr√≥xima a ter seu conte√∫do recuperado
# next_page_url = 'page-1.html'
# while next_page_url:
    # Busca o conte√∫do da pr√≥xima p√°gina
    # response = requests.get(URL_BASE + next_page_url)
    # selector = Selector(text=response.text)
    # Imprime os produtos de uma determinada p√°gina
    # for product in selector.css(".product_pod"):
        # Busca e extrai o t√≠tulo e  o pre√ßo
        # title = product.css("h3 a::attr(title)").get()
        price = product.css(".product_price .price_color::text").re(r"¬£\d+\.\d{2}")
        # print(title, price)

        # Busca o detalhe de um produto
        # detail_href = product.css("h3 a::attr(href)").get()
        # detail_page_url = URL_BASE + detail_href

        # Baixa o conte√∫do da p√°gina de detalhes
        # detail_response = requests.get(detail_page_url)
        # detail_selector = Selector(text=detail_response.text)

        # Extrai a descri√ß√£o do produto
        # description = detail_selector.css("#product_description ~ p::text").get()
        suffix = "...more"
        if description.endswith(suffix):
            description = description[:-len(suffix)]
        # print(description)

    # Descobre qual √© a pr√≥xima p√°gina
    # next_page_url = selector.css(".next a::attr(href)").get()
```

> üí° Strings em **Python** possuem v√°rios m√©todos para ajudarem nesta limpeza, como por exemplo, o `strip`. Para ler a documenta√ß√£o e procurar esses m√©todos, execute `help(str)` no seu terminal interativo.

#### Fatiamento

Estruturas de dados do tipo sequ√™ncia como listas, tuplas e strings podem ter seus elementos acessados atrav√©s de um √≠ndice.

```language-python
# Recupera o primeiro elemento
[1, 2, 3][0]  # Sa√≠da: 1
```

Podemos tamb√©m definir dois √≠ndices que ser√£o o valor inicial e final de uma subsequ√™ncia da estrutura. Ou tr√™s valores, sendo o √∫ltimo o tamanho do passo que daremos ao percorrer a subsequ√™ncia.

```language-python
# Quando n√£o incluso o valor inicial, iniciaremos do √≠ndice 0
# e do √≠ndice 2 em diante, os elementos n√£o s√£o inclu√≠dos
(1, 2, 3, 4)[:2]  # Sa√≠da: (1, 2)

# Quando omitimos o valor final
# o fatiamento ocorre at√© o fim da sequ√™ncia
(1, 2, 3, 4)[1:]  # Sa√≠da: (2, 3, 4)

# V√° do √≠ndice 3 at√© o 7.
# O item no √≠ndice 7 n√£o √© inclu√≠do
"palavra"[3:7]  # Sa√≠da: "avra"

# Come√ßando do elemento de √≠ndice 1, v√° at√© o √∫ltimo,
# saltando de 2 em 2
[1, 2, 3, 4][1::2]  # Sa√≠da: [2, 4]
```

Chamamos esta opera√ß√£o de **fatiamento** e √© muito utilizada para obtermos uma subsequ√™ncia de uma sequ√™ncia.

> üí° √Ä partir da vers√£o 3.9 do Python, teremos uma fun√ß√£o chamada `removesuffix`, que √© equivalente a `palavra[:-len(suffix)]`.

### Banco de Dados

<%= figure(%{src: "/computer-science/python/raspagem-dados/images/python-loves-mongo.jpeg", class: "text-center rounded mx-auto d-block"}) %>

Agora que temos nossos dados, precisamos armazenar esta informa√ß√£o, e para isto utilizaremos o **MongoDB** que, como j√° estudamos, √© um banco de dados de documentos, que armazena dados em formato **JSON** (`BSON`). Precisaremos de uma biblioteca para nos comunicarmos com o sistema de gerenciamento do banco de dados, e a mais popular e robusta √© a `pymongo`. Podemos instal√°-la com o comando:

Lembre-se que para testar o c√≥digo abaixo voc√™ deve criar um ambiente virtual e instalar o `pymongo` com:

```language-bash
$ python3 -m venv .venv && source .venv/bin/activate
$ python3 -m pip install pymongo
```

Ap√≥s a instala√ß√£o vamos ver como podemos realizar a escrita e leitura neste banco de dados. O primeiro passo √© criar uma conex√£o com o banco de dados e isto pode ser feito da seguinte maneira:

> üí° Lembre-se que o MongoDB deve estar preparado para ser acessado do "outro lado" dessa opera√ß√£o!.

```language-python
from pymongo import MongoClient

# Por padr√£o o host √© localhost e porta 27017
# Estes valores podem ser modificados passando uma URI
# client = MongoClient("mongodb://localhost:27017/")
client = MongoClient()
```

Em posse da conex√£o podemos acessar um banco de dados e posteriormente uma cole√ß√£o:

```language-python
from pymongo import MongoClient

client = MongoClient()
# o banco de dados catalogue ser√° criado se n√£o existir
db = client.catalogue
# a cole√ß√£o books ser√° criada se n√£o existir
students = db.books
client.close()  # fecha a conex√£o com o banco de dados
```

Para adicionarmos documentos √† nossa cole√ß√£o utilizamos o m√©todo `insert_one`:

```language-python
from pymongo import MongoClient

client = MongoClient()
db = client.catalogue
# book representa um dado obtido na raspagem
book = {
    "title": "A Light in the Attic",
}
document_id = db.books.insert_one(book).inserted_id
print(document_id)
client.close()  # fecha a conex√£o com o banco de dados
```

Quando um documento √© inserido, um `_id` √∫nico √© gerado e retornado.

Tamb√©m podemos fazer inser√ß√£o de m√∫ltiplos documentos de uma vez.

```language-python
from pymongo import MongoClient

client = MongoClient()
db = client.catalogue
documents = [
    {
        "title": "A Light in the Attic",
    },
    {
        "title": "Tipping the Velvet",
    },
    {
        "title": "Soumission",
    },
]
db.books.insert_many(documents)
client.close()  # fecha a conex√£o com o banco de dados
```

Buscas podem ser feitas utilizando os m√©todos `find` ou `find_one`.

```language-python
from pymongo import MongoClient

client = MongoClient()
db = client.catalogue
# busca um documento da cole√ß√£o, sem filtros
print(db.books.find_one())
# busca utilizando filtros
for book in db.books.find({"title": {"$regex": "t"}}):
    print(book["title"])
client.close()  # fecha a conex√£o com o banco de dados
```

O nosso cliente √© um gerenciador de contexto (_with_), logo podemos utiliz√°-lo como tal, evitando problemas com o fechamento da conex√£o com o banco de dados:

```language-python
from pymongo import MongoClient


with MongoClient() as client:
    db = client.database
    for book in db.books.find({"title": {"$regex": "t"}}):
        print(book["title"])
```

üçÉ A interface de m√©todos desta biblioteca √© t√£o natural que parece que estamos no _shell_ do Mongo. Aqui foram apresentados apenas alguns m√©todos, mas a similaridade √© suficiente para conseguirmos aplicar nosso conhecimento pr√©vio, consultando pontualmente a documenta√ß√£o do pr√≥prio [**MongoDB**](https://docs.mongodb.com/).

### B√¥nus

#### Scrapy

üï∑ Uma excelente e poderosa ferramenta para raspagem de dados √© a [Scrapy](https://scrapy.org/). Ela possui, em sua implementa√ß√£o, todos mecanismos citados anteriormente e outros recursos adicionais.

Vale a pena dar uma olhadinha! üòâ

---

## Vamos fazer juntos

###### Tempo sugerido para realiza√ß√£o: 80 minutos

Agora que j√° passamos pelos conceitos mais b√°sicos, est√° na hora de colocarmos a m√£o na massa com a nossa aula ao vivo!

Bora pro Slack, onde o link estar√° dispon√≠vel. üòâ

---

## Exerc√≠cios

###### Tempo sugerido para realiza√ß√£o: 140 minutos

<%= versioning_your_code(@conn) %>

---

### Agora a pr√°tica

Vamos colocar tudo o que vimos at√© agora em pr√°tica.

---

**Exerc√≠cio 1** Fa√ßa uma requisi√ß√£o ao site `https://httpbin.org/encoding/utf8` e exiba seu conte√∫do de forma leg√≠vel.

**Exerc√≠cio 2** Fa√ßa uma requisi√ß√£o ao recurso usu√°rios da API do Github (`https://api.github.com/users`), exibindo o username e url de todos os usu√°rios retornados.

```language-md
mojombo https://api.github.com/users/mojombo
defunkt https://api.github.com/users/defunkt
pjhyett https://api.github.com/users/pjhyett
wycats https://api.github.com/users/wycats
...
```

**Exerc√≠cio 3** √Äs vezes, voc√™ precisa fazer com que o seu raspador da Web pare√ßa estar fazendo solicita√ß√µes HTTP como o navegador, para que o servidor retorne os mesmos dados que voc√™ v√™ no seu navegador. Fa√ßa uma requisi√ß√£o a `https://scrapethissite.com/pages/advanced/?gotcha=headers` e verifique se foi bem sucedido.

Para verificar se foi bem sucedido, fa√ßa `assert "bot detected" not in response.text`, se nada acontecer, seu c√≥digo est√° funcionando.

ü¶ú Fa√ßa a inspe√ß√£o de como a requisi√ß√£o √© feita pelo navegador para conseguir replicar atrav√©s do c√≥digo.

**Exerc√≠cio 4** Baseados em uma p√°gina contendo detalhes sobre um livro `http://books.toscrape.com/catalogue/the-grand-design_405/index.html`, fa√ßa a extra√ß√£o dos campos t√≠tulo, pre√ßo, descri√ß√£o e url contendo a imagem de capa do livro.

O pre√ßo deve vir somente valores num√©ricos e ponto. Ex: `√Ç¬£13.76` -> `13.76`.

A descri√ß√£o de ter o sufixo "more..." removido quando existir.

Os campos extra√≠dos devem ser apresentados separados por v√≠rgula. Ex: t√≠tulo,pre√ßo,descri√ß√£o,capa.

_Exemplo:_

```language-md
The Grand Design,13.76,THE FIRST MAJOR WORK IN NEARLY A DECADE BY ONE OF THE WORLD√¢S GREAT THINKERS√¢A MARVELOUSLY CONCISE BOOK WITH NEW ANSWERS TO THE ULTIMATE QUESTIONS OF LIFEWhen and howdid the universe begin? Why are we here? Why is there something rather than nothing? What is the nature of reality? Why are the laws of nature so finely tuned as to allow for the existenceof beings like ours THE FIRST MAJOR WORK IN NEARLY A DECADE BY ONE OF THE WORLD√¢S GREAT THINKERS√¢A MARVELOUSLY CONCISE BOOK WITH NEW ANSWERS TO THE ULTIMATE QUESTIONS OF LIFE√Ç¬†When and howdid the universe begin? Why are we here? Why is there something rather than nothing? What is the nature of reality? Why are the laws of nature so finely tuned as to allow for the existenceof beings like ourselves? And, finally, is the apparent √¢grand design√¢or does science offer another explanation? The most fundamental questions about the origins of the universe and of lifeitself, once the province of philosophy, now occupy the territory where scientists, philosophers, and theologians meet√¢if only to disagree. In their new book, Stephen Hawking and LeonardMlodinow present the most recent scientific thinking about the mysteries of the universe, in nontechnical language marked by both brilliance and simplicity. In The Grand Design they explainthat according to quantum theory, the cosmos does not have just a single existence or history, but rather that every possible history of the universe exists simultaneously. When applied tothe universe as a whole, this idea calls into question the very notion of cause and effect. But the √¢top-down√¢multiverse√¢the idea that ours is just one of many universes that appearedspontaneously out of nothing, each with different laws of nature.Along the way Hawking and Mlodinow question the conventional concept of reality, posing a √¢model-dependent√¢theory ofeverything.√¢and provoke√¢like no other. ,http://books.toscrape.com/catalogue/../../media/cache/9b/69/9b696c2064d6ee387774b6121bb4be91.jpg
```

**Exerc√≠cio 5** Modifique o exerc√≠cio anterior para retornar tamb√©m quantos livros est√£o dispon√≠veis apresentando como √∫ltimo campo no retorno.

_Exemplo:_

```language-md
The Grand Design,13.76,THE FIRST MAJOR WORK IN NEARLY A DECADE BY ONE OF THE WORLD√¢S GREAT THINKERS√¢A MARVELOUSLY CONCISE BOOK WITH NEW ANSWERS TO THE ULTIMATE QUESTIONS OF LIFEWhen and howdid the universe begin? Why are we here? Why is there something rather than nothing? What is the nature of reality? Why are the laws of nature so finely tuned as to allow for the existenceof beings like ours THE FIRST MAJOR WORK IN NEARLY A DECADE BY ONE OF THE WORLD√¢S GREAT THINKERS√¢A MARVELOUSLY CONCISE BOOK WITH NEW ANSWERS TO THE ULTIMATE QUESTIONS OF LIFE√Ç¬†When and howdid the universe begin? Why are we here? Why is there something rather than nothing? What is the nature of reality? Why are the laws of nature so finely tuned as to allow for the existenceof beings like ourselves? And, finally, is the apparent √¢grand design√¢or does science offer another explanation? The most fundamental questions about the origins of the universe and of lifeitself, once the province of philosophy, now occupy the territory where scientists, philosophers, and theologians meet√¢if only to disagree. In their new book, Stephen Hawking and LeonardMlodinow present the most recent scientific thinking about the mysteries of the universe, in nontechnical language marked by both brilliance and simplicity. In The Grand Design they explainthat according to quantum theory, the cosmos does not have just a single existence or history, but rather that every possible history of the universe exists simultaneously. When applied tothe universe as a whole, this idea calls into question the very notion of cause and effect. But the √¢top-down√¢multiverse√¢the idea that ours is just one of many universes that appearedspontaneously out of nothing, each with different laws of nature.Along the way Hawking and Mlodinow question the conventional concept of reality, posing a √¢model-dependent√¢theory ofeverything.√¢and provoke√¢like no other. ,http://books.toscrape.com/catalogue/../../media/cache/9b/69/9b696c2064d6ee387774b6121bb4be91.jpg,5
```

‚ùó Importe o arquivo [books.json](/computer-science/python/entrada-saida/books.json) {: .external-link target="_blank" rel="noreferrer noopener" } no MongoDB antes de responder as pr√≥ximas quest√µes.

ü¶ú `mongoimport --db library books.json`

**Exerc√≠cio 6** Escreva um programa que se conecte ao banco de dados `library` e liste os livros da cole√ß√£o `books` para uma determinada categoria recebida por uma pessoa usu√°ria. Somente o t√≠tulo dos livros deve ser exibido.

**Exerc√≠cio 7** Fa√ßa o calculo de quantos livros publicados (`STATUS = PUBLISH`) temos em nosso banco de dados por categoria. Ordenando-os de forma decrescente de acordo com a quantidade.

ü¶ú Voc√™ pode utilizar `agreggation framework` para auxiliar neste exerc√≠cio.

_Sa√≠da:_

```language-md
Java 95
Internet 41
Microsoft .NET 33
Web Development 16
Software Engineering 15
Business 12
Programming 12
Client-Server 11
Microsoft 8
Theory 7
...
```

### B√¥nus

**Exerc√≠cio 8** Agora um desafio, v√° ao site `https://en.wikipedia.org/wiki/Gallery_of_sovereign_state_flags` e recupere as imagens de todas as bandeiras.

ü¶ú Fa√ßa requisi√ß√µes para as URLs das imagens e escreva seus conte√∫dos em arquivos bin√°rios. S√£o 206 ao total.

**Exerc√≠cio 9** Alguns sites possuem pagina√ß√£o feita atrav√©s de rolagem infinita. Descubra como funciona a rolagem infinita e extraia todas as cita√ß√µes do seguinte site: `http://quotes.toscrape.com/scroll`.

ü¶ú S√£o 100 cita√ß√µes no total.

## Recursos adicionais (opcional)

- [An√°lise de dados p√∫blicos - Professor Masanori](https://www.youtube.com/playlist?list=PLUukMN0DTKCu6g2Lq1KXLnIX6Ilk4DAPI) {: .external-link target="_blank" rel="noreferrer noopener" }

- [Requests/Web scraping](https://youtu.be/geGjMToK5u8) {: .external-link target="_blank" rel="noreferrer noopener" }

- [Conhecendo XPATH com Renne Rocha](https://youtu.be/vuLNc2yCNYk) {: .external-link target="_blank" rel="noreferrer noopener" }

- [Criando Web Crawlers com Scrapy e Selenium para p√°ginas com Javascript - Gileno Filho](https://www.youtube.com/watch?v=AXSo4kBAygE) {: .external-link target="_blank" rel="noreferrer noopener" }

- [[PyBR14] Web crawling e scraping com Scrapy e Scrapy Cloud - Lidiane Taquehara](https://youtu.be/vmRfO2uULfw) {: .external-link target="_blank" rel="noreferrer noopener" }

- [Scrapy Video Tutorials](https://www.scrapinghub.com/learn-scrapy/) {: .external-link target="_blank" rel="noreferrer noopener" }

---

## Pr√≥ximo

<%= next_button(@conn) %>
